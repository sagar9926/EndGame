{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar9926/EndGame/blob/master/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPxm5D4XaIIE",
        "colab_type": "code",
        "outputId": "e820a9bd-b3b6-4ab3-8f14-bdd2a712440b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!git clone https://github.com/sagar9926/project10.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'project10'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/111)\u001b[K\rremote: Counting objects:   1% (2/111)\u001b[K\rremote: Counting objects:   2% (3/111)\u001b[K\rremote: Counting objects:   3% (4/111)\u001b[K\rremote: Counting objects:   4% (5/111)\u001b[K\rremote: Counting objects:   5% (6/111)\u001b[K\rremote: Counting objects:   6% (7/111)\u001b[K\rremote: Counting objects:   7% (8/111)\u001b[K\rremote: Counting objects:   8% (9/111)\u001b[K\rremote: Counting objects:   9% (10/111)\u001b[K\rremote: Counting objects:  10% (12/111)\u001b[K\rremote: Counting objects:  11% (13/111)\u001b[K\rremote: Counting objects:  12% (14/111)\u001b[K\rremote: Counting objects:  13% (15/111)\u001b[K\rremote: Counting objects:  14% (16/111)\u001b[K\rremote: Counting objects:  15% (17/111)\u001b[K\rremote: Counting objects:  16% (18/111)\u001b[K\rremote: Counting objects:  17% (19/111)\u001b[K\rremote: Counting objects:  18% (20/111)\u001b[K\rremote: Counting objects:  19% (22/111)\u001b[K\rremote: Counting objects:  20% (23/111)\u001b[K\rremote: Counting objects:  21% (24/111)\u001b[K\rremote: Counting objects:  22% (25/111)\u001b[K\rremote: Counting objects:  23% (26/111)\u001b[K\rremote: Counting objects:  24% (27/111)\u001b[K\rremote: Counting objects:  25% (28/111)\u001b[K\rremote: Counting objects:  26% (29/111)\u001b[K\rremote: Counting objects:  27% (30/111)\u001b[K\rremote: Counting objects:  28% (32/111)\u001b[K\rremote: Counting objects:  29% (33/111)\u001b[K\rremote: Counting objects:  30% (34/111)\u001b[K\rremote: Counting objects:  31% (35/111)\u001b[K\rremote: Counting objects:  32% (36/111)\u001b[K\rremote: Counting objects:  33% (37/111)\u001b[K\rremote: Counting objects:  34% (38/111)\u001b[K\rremote: Counting objects:  35% (39/111)\u001b[K\rremote: Counting objects:  36% (40/111)\u001b[K\rremote: Counting objects:  37% (42/111)\u001b[K\rremote: Counting objects:  38% (43/111)\u001b[K\rremote: Counting objects:  39% (44/111)\u001b[K\rremote: Counting objects:  40% (45/111)\u001b[K\rremote: Counting objects:  41% (46/111)\u001b[K\rremote: Counting objects:  42% (47/111)\u001b[K\rremote: Counting objects:  43% (48/111)\u001b[K\rremote: Counting objects:  44% (49/111)\u001b[K\rremote: Counting objects:  45% (50/111)\u001b[K\rremote: Counting objects:  46% (52/111)\u001b[K\rremote: Counting objects:  47% (53/111)\u001b[K\rremote: Counting objects:  48% (54/111)\u001b[K\rremote: Counting objects:  49% (55/111)\u001b[K\rremote: Counting objects:  50% (56/111)\u001b[K\rremote: Counting objects:  51% (57/111)\u001b[K\rremote: Counting objects:  52% (58/111)\u001b[K\rremote: Counting objects:  53% (59/111)\u001b[K\rremote: Counting objects:  54% (60/111)\u001b[K\rremote: Counting objects:  55% (62/111)\u001b[K\rremote: Counting objects:  56% (63/111)\u001b[K\rremote: Counting objects:  57% (64/111)\u001b[K\rremote: Counting objects:  58% (65/111)\u001b[K\rremote: Counting objects:  59% (66/111)\u001b[K\rremote: Counting objects:  60% (67/111)\u001b[K\rremote: Counting objects:  61% (68/111)\u001b[K\rremote: Counting objects:  62% (69/111)\u001b[K\rremote: Counting objects:  63% (70/111)\u001b[K\rremote: Counting objects:  64% (72/111)\u001b[K\rremote: Counting objects:  65% (73/111)\u001b[K\rremote: Counting objects:  66% (74/111)\u001b[K\rremote: Counting objects:  67% (75/111)\u001b[K\rremote: Counting objects:  68% (76/111)\u001b[K\rremote: Counting objects:  69% (77/111)\u001b[K\rremote: Counting objects:  70% (78/111)\u001b[K\rremote: Counting objects:  71% (79/111)\u001b[K\rremote: Counting objects:  72% (80/111)\u001b[K\rremote: Counting objects:  73% (82/111)\u001b[K\rremote: Counting objects:  74% (83/111)\u001b[K\rremote: Counting objects:  75% (84/111)\u001b[K\rremote: Counting objects:  76% (85/111)\u001b[K\rremote: Counting objects:  77% (86/111)\u001b[K\rremote: Counting objects:  78% (87/111)\u001b[K\rremote: Counting objects:  79% (88/111)\u001b[K\rremote: Counting objects:  80% (89/111)\u001b[K\rremote: Counting objects:  81% (90/111)\u001b[K\rremote: Counting objects:  82% (92/111)\u001b[K\rremote: Counting objects:  83% (93/111)\u001b[K\rremote: Counting objects:  84% (94/111)\u001b[K\rremote: Counting objects:  85% (95/111)\u001b[K\rremote: Counting objects:  86% (96/111)\u001b[K\rremote: Counting objects:  87% (97/111)\u001b[K\rremote: Counting objects:  88% (98/111)\u001b[K\rremote: Counting objects:  89% (99/111)\u001b[K\rremote: Counting objects:  90% (100/111)\u001b[K\rremote: Counting objects:  91% (102/111)\u001b[K\rremote: Counting objects:  92% (103/111)\u001b[K\rremote: Counting objects:  93% (104/111)\u001b[K\rremote: Counting objects:  94% (105/111)\u001b[K\rremote: Counting objects:  95% (106/111)\u001b[K\rremote: Counting objects:  96% (107/111)\u001b[K\rremote: Counting objects:  97% (108/111)\u001b[K\rremote: Counting objects:  98% (109/111)\u001b[K\rremote: Counting objects:  99% (110/111)\u001b[K\rremote: Counting objects: 100% (111/111)\u001b[K\rremote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 111 (delta 58), reused 29 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (111/111), 3.81 MiB | 7.18 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOVo_HUuacvq",
        "colab_type": "code",
        "outputId": "4ed8ab3d-9463-4bd9-c3e3-2b4dcb4b0cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd project10/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/project10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C50jet2yaoD9",
        "colab_type": "code",
        "outputId": "4f26a7e5-86e5-414e-de3f-01025ae3d51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.png\t      AIUpdated_steplr.py  destination.jpg   MASK1.png\t pytorch_models\n",
            "AI.py\t      car.png\t\t   manualcabrot.py   mask.png\t results\n",
            "AIUpdated.py  citymap.png\t   manualcarrot1.py  models1.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h86bzcRibvUR",
        "colab_type": "code",
        "outputId": "2ab4f3c9-4320-4ac4-d4d1-297e941c8900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 223kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IWjSxJQFHJK",
        "colab_type": "code",
        "outputId": "08a960f0-3f75-4c40-f45c-b9ce4405cd9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggbyJrr4a3f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cd /content/gdrive/My Drive/EndGame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXDRD05OFTNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cd /content/gdrive/My Drive/EndGame/results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHKaAmkKF4M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir pytorch_modelsOldLaptop\n",
        "#!mkdir resultsOldLaptop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9fj1g9mXUFz",
        "colab_type": "code",
        "outputId": "2780dae0-a961-4382-e8f8-b8813a3f53fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.png\t      AIUpdated_steplr.py  destination.jpg   MASK1.png\t pytorch_models\n",
            "AI.py\t      car.png\t\t   manualcabrot.py   mask.png\t results\n",
            "AIUpdated.py  citymap.png\t   manualcarrot1.py  models1.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWYe56Hec9c6",
        "colab_type": "code",
        "outputId": "8ef59369-8150-4f25-bd1a-1d9f12680ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adoATo8sfwX2",
        "colab_type": "code",
        "outputId": "54dc9cd0-526a-4234-babc-b5839254d1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "##Best Results\n",
        "\n",
        "%%writefile manualcarrot1.py\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import math \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import pygame\n",
        "from math import sin, radians, degrees, copysign\n",
        "from pygame.math import Vector2\n",
        "\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "from AIUpdatedV2 import ReplayBuffer, TD3\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "\n",
        "seed = 0  # Random seed number\n",
        "# Set seed for consistency\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "save_models = True  # Boolean checker whether or not to save the pre-trained model\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", \"CarApp\", str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")\n",
        "\n",
        "\n",
        "prev_reward = 0\n",
        "origin_x = 743\n",
        "origin_y = 380\n",
        "coordinates = [[590,380],[730,90],[1130,405],[120,300],[1110,630]]\n",
        "first_update = True # Setting the first update\n",
        "last_distance = 0   # Initializing the last distance\n",
        "\n",
        "def init():\n",
        "    global sand\n",
        "    global dest_x\n",
        "    global dest_y\n",
        "    global origin_x\n",
        "    global origin_y\n",
        "    global first_update\n",
        "    sand = np.zeros((longueur, largeur))\n",
        "    img = PILImage.open(\"mask.png\").convert('L')\n",
        "    sand = np.asarray(img) / 255\n",
        "    dest_x, dest_y = coordinates[np.random.randint(0, 5)]\n",
        "    first_update = False\n",
        "\n",
        "\n",
        "class Car:\n",
        "    cropsize = 28\n",
        "    padsize = 28\n",
        "    rotation = 0\n",
        "    state_img_patch = np.zeros([1,int(cropsize),int(cropsize)])\n",
        "    \n",
        "    def __init__(self, x=origin_x, y=origin_y, angle=0.0):\n",
        "        # Positon vector for Car initialised with the initial location of car on map\n",
        "        self.position = Vector2(743,358) \n",
        "        \n",
        "        # Velocity Vector of car\n",
        "        self.velocity = Vector2(0.0, 0.0)\n",
        "        \n",
        "        # Steering angle \n",
        "        self.angle = angle\n",
        "        \n",
        "        # 28x28 patch to be fed into model\n",
        "        self.state_img_patch = np.zeros([1,int(28),int(28)])\n",
        "        \n",
        "        # 56x56 patch to display the patch on the game screen\n",
        "        self.state_img_patch2 = np.zeros([1,int(56),int(56)])\n",
        "\n",
        "    def move(self, rotation):\n",
        "        global episode_num\n",
        "        global padsize\n",
        "        global cropsize\n",
        "        \n",
        "        # In pygame Top left corner is the origin thus inverting Y axis by subtracting it from Sand image height\n",
        "        #storing the car location\n",
        "        tempx,tempy = self.position.x , 660 - self.position.y\n",
        "        \n",
        "        # Updating the car location\n",
        "        self.position = Vector2(*self.velocity) + self.position\n",
        "        self.rotation = rotation\n",
        "        \n",
        "        # Updating the steering angle\n",
        "        self.angle = self.angle + self.rotation\n",
        "        \n",
        "        #Creating Numpy array for sand\n",
        "        Sand = np.copy(sand)\n",
        "        \n",
        "        #Padding the sand image\n",
        "        Sand = np.pad(Sand,self.padsize,constant_values=1.0,mode = 'constant')\n",
        "        \n",
        "        # Cropping a patch of size 56x56 at the current location of car on sand map\n",
        "        Sand = Sand[int(tempx) - self.cropsize + self.padsize:int(tempx) + self.cropsize + self.padsize,\n",
        "                   int(tempy) - self.cropsize + self.padsize:int(tempy) + self.cropsize + self.padsize]\n",
        "        \n",
        "        # Orienting the cropped patch along the orientation of velocity vector of car\n",
        "        Sand = rotate(Sand, angle=90-(self.angle-90), reshape= False, order=1, mode='constant',  cval=1.0)\n",
        "        \n",
        "        #setting two pixels as black and white respectively to locate the car on cropped sand patch\n",
        "        Sand[int(self.padsize)-5:int(self.padsize), int(self.padsize) - 2:int(self.padsize) + 3 ] = 0\n",
        "        Sand[int(self.padsize):int(self.padsize) + 5, int(self.padsize) - 2:int(self.padsize) + 3] = 1\n",
        "        \n",
        "        self.state_img_patch2=Sand\n",
        "        \n",
        "        # Second smaller crop\n",
        "        y,x = Sand.shape\n",
        "        startx = x//2 - (self.cropsize//2)\n",
        "        starty = y//2 - (self.cropsize//2)\n",
        "        Sand = Sand[starty:starty+self.cropsize,startx:startx+self.cropsize]\n",
        "        \n",
        "        self.state_img_patch=Sand\n",
        "        \n",
        "        \n",
        "        # Resizing the cropped image patch to 28x28\n",
        "        #self.state_img_patch = self.state_img_patch[::2, ::2]\n",
        "        self.state_img_patch2 = np.expand_dims(self.state_img_patch2, 0)\n",
        "        self.state_img_patch = np.expand_dims(self.state_img_patch, 0)\n",
        "        \n",
        "        return 90-(self.angle-90)\n",
        "\n",
        "\n",
        "class Game:\n",
        "    def __init__(self):\n",
        "        pygame.init()\n",
        "        pygame.display.set_caption(\"Car tutorial\")\n",
        "        width = 1200\n",
        "        height = 660\n",
        "        self.screen = pygame.display.set_mode((width, height))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.ticks = 60\n",
        "        self.exit = False\n",
        "        self.car = Car()\n",
        "        #self.bg_img = pygame.image.load('MASK1.png')\n",
        "        #self.bg_img = pygame.transform.scale(self.bg_img,(1200,660))\n",
        "        self.city_img = pygame.image.load('citymap.png')\n",
        "        self.city_img = pygame.transform.scale(self.city_img,(1200,690))\n",
        "        \n",
        "        self.car_img = pygame.image.load('car.png')\n",
        "        self.car_img = pygame.transform.scale(self.car_img,(20,15))\n",
        "        \n",
        "        self.dest_img = pygame.image.load('destination.jpg')\n",
        "        self.dest_img = pygame.transform.scale(self.dest_img,(20,20))\n",
        "        \n",
        "        global total_timesteps\n",
        "    \n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        global last_distance\n",
        "        global origin_x\n",
        "        global origin_y\n",
        "        self.car.position.x = origin_x\n",
        "        self.car.position.y  = 660 - origin_y\n",
        "        x_dist = dest_x - self.car.position.x\n",
        "        y_dist = dest_y - (660 - self.car.position.y)\n",
        "        \n",
        "        # Calculating the angle between the velocity vector and distance displacement vector in degrees\n",
        "        Int_angle = -(180 / math.pi) * math.atan2(\n",
        "            self.car.velocity[0] * y_dist+ self.car.velocity[1] * x_dist,\n",
        "            self.car.velocity[0]* x_dist - self.car.velocity[1] * y_dist)\n",
        "        \n",
        "        # converting it into radians\n",
        "        orientation = Int_angle/180\n",
        "        \n",
        "        # Calculate the distance of car current position w.r.t Destination\n",
        "        self.distance = np.sqrt((self.car.position.x- dest_x) ** 2 + ( 660 - self.car.position.y - dest_y) ** 2)\n",
        "        \n",
        "        state = [self.car.state_img_patch , orientation, -orientation, last_distance - self.distance]\n",
        "        return state\n",
        "\n",
        "\n",
        "    def step(self,action):\n",
        "        global dest_x\n",
        "        global dest_y\n",
        "        global origin_x\n",
        "        global origin_y\n",
        "        global done\n",
        "        global last_distance\n",
        "        global distance_travelled\n",
        "\n",
        "        rotation = action.item()\n",
        "        tAngle = self.car.move(rotation)\n",
        "        self.Game_Screen(dest_x,dest_y,tAngle)\n",
        "        self.distance = np.sqrt((self.car.position.x - dest_x) ** 2 + ( 660 - self.car.position.y - dest_y) ** 2)\n",
        "        x_dist = dest_x - self.car.position.x\n",
        "        y_dist = dest_y - 660 + self.car.position.y\n",
        "        Int_angle = -(180 / math.pi) * math.atan2(\n",
        "            self.car.velocity[0] * y_dist+ self.car.velocity[1] * x_dist,\n",
        "            self.car.velocity[0]* x_dist - self.car.velocity[1] * y_dist)\n",
        "        orientation = Int_angle / 180.\n",
        "        \n",
        "        # State vector \n",
        "        state = [self.car.state_img_patch, orientation, -orientation, last_distance-self.distance]\n",
        "        \n",
        "        #Penalty for moving on sand\n",
        "        if sand[int(self.car.position.x), int(660 - self.car.position.y)] > 0:\n",
        "            self.car.velocity = Vector2(1, 0).rotate(self.car.angle)\n",
        "            prev_reward = -5 #-1\n",
        "\n",
        "        else:  # Living Penalty\n",
        "            self.car.velocity = Vector2(2.5, 0).rotate(self.car.angle)\n",
        "            prev_reward = -1.5 \n",
        "            # Reward for moving on road and towards destination\n",
        "            if self.distance < last_distance:\n",
        "                prev_reward = 1.5 \n",
        "                \n",
        "        # Boundary Conditions\n",
        "        if self.car.position.x < 5:\n",
        "        \n",
        "            self.car.position.x = 5\n",
        "            prev_reward = -10 #-1\n",
        "            \n",
        "        if self.car.position.x > self.width - 5:\n",
        "            self.car.position.x= self.width - 5\n",
        "            prev_reward = -10    #-1\n",
        "            \n",
        "        if self.car.position.y < 5:\n",
        "            self.car.position.y = 5\n",
        "            prev_reward = -10   #-1\n",
        "            \n",
        "        if self.car.position.y > self.height - 5:\n",
        "            self.car.position.y = self.height - 5\n",
        "            prev_reward = -10    #-1\n",
        "            \n",
        "        # Reward fro reaching destination\n",
        "        if self.distance < 30:\n",
        "            origin_x = dest_x\n",
        "            origin_y = dest_y\n",
        "            dest_x,dest_y= coordinates[np.random.randint(0,5)]\n",
        "            prev_reward = 100\n",
        "            done = True\n",
        "        print(prev_reward)\n",
        "        last_distance = self.distance\n",
        "        return state, prev_reward, done\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_policy(self, policy, eval_episodes=10):\n",
        "        avg_reward = 0.\n",
        "        for _ in range(eval_episodes):\n",
        "            obs = self.reset() # ToDo reset env\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = policy.select_action(np.array(obs))\n",
        "                obs,reward,done = self.step(action)\n",
        "                avg_reward += reward\n",
        "        avg_reward /= eval_episodes\n",
        "        print(\"---------------------------------------\")\n",
        "        print(\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "        print(\"---------------------------------------\")\n",
        "        return avg_reward\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, dt):\n",
        "        global first_update\n",
        "        global dest_x\n",
        "        global dest_y\n",
        "        global longueur\n",
        "        global largeur\n",
        "        global prev_reward\n",
        "        global reward\n",
        "        global policy\n",
        "        global done\n",
        "        global episode_reward\n",
        "        global replay_buffer\n",
        "        global obs\n",
        "        global new_obs\n",
        "        global evaluations\n",
        "\n",
        "        global episode_num\n",
        "        global total_timesteps\n",
        "        global timesteps_since_eval\n",
        "        global episode_num\n",
        "        global max_timesteps\n",
        "        global max_episode_steps\n",
        "        global episode_timesteps\n",
        "        global distance_travelled\n",
        "        self.width = 1200\n",
        "        self.height = 660\n",
        "        longueur = self.width\n",
        "        largeur = self.height\n",
        "        if first_update:\n",
        "            init()\n",
        "            evaluations = [self.evaluate_policy(policy)]\n",
        "            distance_travelled=0\n",
        "            done = True\n",
        "            obs = self.reset()\n",
        "        \n",
        "        if episode_reward<-2500: # if total accumulated reward becomes more negetive than -2500 then the episode is completed\n",
        "            done=True\n",
        "        if total_timesteps < max_timesteps:\n",
        "            if done:\n",
        "                print(\"done-reached\")\n",
        "                # If we are not at the very beginning, we start the training process of the model\n",
        "                if total_timesteps != 0:\n",
        "                    print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num,\n",
        "                                                                                  episode_reward))\n",
        "                    policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip,\n",
        "                                 policy_freq)\n",
        "\n",
        "                # We evaluate the episode and we save the policy\n",
        "                if timesteps_since_eval >= eval_freq:\n",
        "                    print(\"Reached-\")\n",
        "                    timesteps_since_eval %= eval_freq\n",
        "                    evaluations.append(self.evaluate_policy(policy))\n",
        "                    policy.save(file_name, directory=\"./pytorch_modelsOldLaptop\")\n",
        "                    np.save(\"./resultsOldLaptop/%s\" % (file_name), evaluations)\n",
        "\n",
        "                # When the training step is done, we reset the state of the environment\n",
        "                obs = self.reset()\n",
        "\n",
        "                # Set the Done to False\n",
        "                done = False\n",
        "\n",
        "                # Set rewards and episode timesteps to zero\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                episode_num += 1\n",
        "\n",
        "            # Before 10000 timesteps, we play random actions\n",
        "            if total_timesteps < start_timesteps:\n",
        "                action = np.random.uniform(low=-5, high=5, size=(1,))\n",
        "            else:  # After start_timesteps, we switch to the model\n",
        "                action = policy.select_action(np.array(obs))\n",
        "                # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "                if expl_noise != 0:\n",
        "                    action = (action + np.random.normal(0, expl_noise, size=1)).clip(\n",
        "                        -5, 5)\n",
        "\n",
        "            # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "            new_obs,reward, done = self.step(action)\n",
        "\n",
        "            # We check if the episode is done\n",
        "            done_bool = 0 if episode_timesteps + 1 == max_episode_steps else float(\n",
        "                done)\n",
        "\n",
        "            # We increase the total reward\n",
        "            episode_reward += reward\n",
        "            # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "            replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "            #if total_timesteps%10==1:\n",
        "              #print(\" \".join([str(total_timesteps), str(obs[1:]), str(new_obs[1:]), str(action), str(reward), str(done_bool)]))\n",
        "            # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "            obs = new_obs\n",
        "            episode_timesteps += 1\n",
        "            total_timesteps += 1\n",
        "            timesteps_since_eval += 1\n",
        "            # Saving model at every 5000 iterations\n",
        "            if total_timesteps%5000==1:\n",
        "                print(\"Saving Model %s\" % (file_name))\n",
        "                policy.save(\"%s\" % (file_name), directory=\"./pytorch_modelsOldLaptop\")\n",
        "                np.save(\"./resultsOldLaptop/%s\" % (file_name), evaluations)\n",
        "        else:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            new_obs,reward, done = self.step(action)\n",
        "            obs = new_obs\n",
        "            total_timesteps += 1\n",
        "            if total_timesteps%1000==1:\n",
        "                print(total_timesteps)\n",
        "\n",
        "\n",
        "    def Game_Screen(self,dest_x,dest_y,tAngle):\n",
        "        \n",
        "        # Creating an image to display the state patch going into model \n",
        "        display_cam = np.copy(self.car.state_img_patch2.squeeze())\n",
        "        display_cam2 = np.zeros([int(56),int(56),3])\n",
        "        display_cam2[:,:,0] = display_cam*255\n",
        "        display_cam2[:,:,1] = display_cam*255\n",
        "        display_cam2[:,:,2] = display_cam*255\n",
        "        display_cam3 = pygame.surfarray.make_surface(display_cam2)\n",
        "        \n",
        "        # Steering the car\n",
        "        rot_img = pygame.transform.rotate(self.car_img,tAngle)\n",
        "        # display City_map\n",
        "        self.screen.blit(self.city_img, self.city_img.get_rect())\n",
        "        # display car\n",
        "        self.screen.blit(rot_img,self.car.position)\n",
        "        # display destination\n",
        "        self.screen.blit(self.dest_img,(dest_x, 660 - dest_y))\n",
        "        # display the 28x28  state patch\n",
        "        self.screen.blit(display_cam3,(400,100)) \n",
        "        pygame.display.flip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initializing Global Variables\n",
        "start_timesteps = 1e4  # 1e4 Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 1e3  #5e3 How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e6  #5e5 Total number of iterations/timesteps\n",
        "\n",
        "expl_noise = 0.1  # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100  # Size of the batch\n",
        "discount = 0.99  # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005  # Target network update rate\n",
        "policy_noise = 0.2  # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5  # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2  #\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "\n",
        "episode_reward=0\n",
        "distance_travelled=0\n",
        "max_episode_steps = 1000\n",
        "done = True # Episode over\n",
        "load_model=True # Inference. Set to false for training from scratch\n",
        "\n",
        "state_dim = 4\n",
        "action_dim = 1\n",
        "max_action = 5\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "\n",
        "obs=np.array([])\n",
        "new_obs=np.array([])\n",
        "evaluations=[]\n",
        "        \n",
        "parent = Game()\n",
        "start_ticks=pygame.time.get_ticks()\n",
        "\n",
        "#parent.run()\n",
        "while True:\n",
        "    pygame.event.get()\n",
        "    parent.update(1/60)\n",
        "    time.sleep(1/60) \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting manualcarrot1.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Y6f6DWV_jf",
        "colab_type": "code",
        "outputId": "780f18a0-cff8-481f-edb7-737063e9d2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.png\t      AIUpdated_steplr.py  destination.jpg   MASK1.png\t __pycache__\n",
            "AI.py\t      car.png\t\t   manualcabrot.py   mask.png\t pytorch_models\n",
            "AIUpdated.py  citymap.png\t   manualcarrot1.py  models1.py  results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRlyI_lFIt1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python manualcarrot1.py"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}